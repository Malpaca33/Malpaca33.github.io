
# 文本处理基础：单词计数与文件合并（含核心代码与思考框架）

## 1. 单词计数（Word Count）

### 为什么要做 💡

文本分析离不开基础统计，单词计数是最底层的一环。  
它能帮助我快速判断文本主题、关键词分布、异常词频等，日志分析和 NLP 处理都会用到。

### 不做会怎样 ✖

- 文本成了“黑箱”，无法分析
    
- 关键词、热点词全靠猜
    
- 语料不干净会直接污染后续处理
    
- 大规模文本完全无法人工判断
    

### 应用场景 ✔

- NLP 语料预处理
    
- 搜索引擎倒排索引构建
    
- 日志关键词提取
    
- 可视化（词云）的基础
    

---

## 核心代码（简洁版）

```java
public static Map<String, Integer> wordCount(String filePath) {
    Map<String, Integer> map = new LinkedHashMap<>();

    try (BufferedReader br = new BufferedReader(new FileReader(filePath))) {
        String line;
        while ((line = br.readLine()) != null) {
            String[] words = line.split("\\s+");
            for (String w : words) {
                if (w.isEmpty()) continue;
                map.put(w, map.getOrDefault(w, 0) + 1);
            }
        }
    } catch (Exception e) {
        throw new RuntimeException(e);
    }
    return map;
}
```

### 代码框架（我重点记住的）

1. **逐行读取**
    
2. **分词（正则 / 空格）**
    
3. **Map 计数（`getOrDefault`）**
    
4. **保持顺序用 LinkedHashMap**
    
5. **大文本只流式处理，不进内存**
    

---

# 2. 文件合并（Merge Files）

### 为什么要合并 💡

日志系统普遍按小时、大小拆分文件，分析的时候必须把碎片拼回去。很多处理流程也是分片产出，最后要统一整合。

### 不合并的影响 ✖

- 分析工具无法跨文件执行
    
- 日志不连续，排查非常痛苦
    
- 大任务的输入数据不完整
    
- 文件碎片越积越多，非常混乱
    

### 应用场景 ✔

- 日志系统（nginx/Tomcat 网关日志）
    
- 文本切片任务的合并
    
- 多模块产出的结果聚合
    
- 大数据处理前的收敛阶段
    

---

## 核心代码（简洁版）

```java
public static void mergeFiles(List<String> paths, String out) {
    try (BufferedWriter bw = new BufferedWriter(new FileWriter(out))) {
        for (String p : paths) {
            try (BufferedReader br = new BufferedReader(new FileReader(p))) {
                String line;
                while ((line = br.readLine()) != null) {
                    bw.write(line);
                    bw.newLine();
                }
            }
        }
    } catch (Exception e) {
        throw new RuntimeException(e);
    }
}
```

### 代码框架（我重点记住的）

1. 输出流只开一次（`BufferedWriter`）
    
2. 多输入文件循环读
    
3. 每行写入 + 换行
    
4. 不能一次性全部读入（大文件会爆）
    
5. 使用 try-with-resources 自动关闭流
    

---

# 我在这些题里踩的坑（反思）

### 1. 类名冲突 ✖

自己写了个 `BufferedReader`，名字和 JDK 冲突。

**结果：**

- 程序递归引用自己写的类
    
- 真正的 `java.io.BufferedReader` 完全用不了
    
- 代码直接炸裂
    

### 2. 强制类型转换乱写 ✖

原来写过 `(Integer) line` 这种完全不可能成立的写法。

**原因：**

- `line` 是字符串
    
- 不是数字，更不是 `Integer`
    

### 3. 异常处理写错 ✖

写过：

```java
throw new LinkedHashMap<>();
```

这是语法错误 + 逻辑错误。

正确方式：

```java
throw new RuntimeException(e);
```

### 4. I/O 思维不成熟 ✔

最开始没有建立“读 → 处理 → 写”的清晰框架，导致代码混乱。

---

# 我的统一思考框架（三步走）

## ① 为什么要做（Motivation）

- 让文本可分析
    
- 做数据清洗
    
- 让大文件能被工具处理
    
- 提高后续处理速度与质量
    

## ② 不做会怎样（Impact）

- 数据冗余
    
- 分析不准确
    
- 任务无法正常执行
    
- 日志排查断点多
    

## ③ 怎么应用（Use Cases）

- 日志
    
- 数据清洗
    
- NLP 预处理
    
- 分片任务合并
    
- 服务排障
    

这个框架可以套在所有文件处理题里（去重、计数、复制、合并、切片）。

---

如果你需要，我可以继续整理：

- 第五题（文件复制）+ NIO 版本 + 零拷贝总结
    
- 整个“文件处理 4 题”合成一个系列文章
    
- 为你的主页生成统一风格的模板文章
    

继续扩展吗？